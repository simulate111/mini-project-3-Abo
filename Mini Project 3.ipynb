{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea5a5b5-7365-4340-b332-52621feac837",
   "metadata": {},
   "source": [
    "Task 1 – Your main task is to use K-Means and DBSCAN to do clustering on the given dataset. Your code needs to consider the following aspects, and this also should be reflected in your final report.\n",
    "\n",
    "• How do you choose the number of clusters in K-Means, is it the same number of clusters for DBSCAN?\n",
    "\n",
    "• How do you find the optimal parameters’ values?\n",
    "\n",
    "• What data processing steps do you apply and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4e9e492-bc6c-4f83-beb2-5243ea902ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/UCI HAR Dataset/UCI HAR Dataset/your_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/UCI HAR Dataset/UCI HAR Dataset/your_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming the dataset contains feature columns and possibly a label column\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Separate features (X) from labels (y), if labels are available\u001b[39;00m\n\u001b[0;32m     11\u001b[0m X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_column_name\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Adjust 'label_column_name' to your dataset's label column\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/UCI HAR Dataset/UCI HAR Dataset/your_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Data/UCI HAR Dataset/UCI HAR Dataset/your_dataset.csv')\n",
    "\n",
    "# Assuming the dataset contains feature columns and possibly a label column\n",
    "# Separate features (X) from labels (y), if labels are available\n",
    "X = data.drop('label_column_name', axis=1)  # Adjust 'label_column_name' to your dataset's label column\n",
    "y = data['label_column_name']  # Adjust 'label_column_name' to your dataset's label column\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize KMeans model\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)  # Adjust the number of clusters as needed\n",
    "\n",
    "# Fit KMeans model to the standardized features\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Get cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Add cluster labels to the original dataset\n",
    "data['cluster'] = cluster_labels\n",
    "\n",
    "# Visualize the clusters (assuming you have 2D data, otherwise you need dimensionality reduction)\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=cluster_labels, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='x', c='red', s=100)\n",
    "plt.title('KMeans Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "995eec12-d51f-46f0-a790-04f76feea799",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBSCAN failed to generate multiple clusters with the given parameter ranges.\n"
     ]
    }
   ],
   "source": [
    "# Parameter tuning for DBSCAN\n",
    "eps_values = [0.1, 0.5, 1.0, 1.5, 2.0]  # Expanded range of epsilon values\n",
    "min_samples_values = [5, 10, 15, 20, 25]  # Expanded range of min_samples values\n",
    "\n",
    "dbscan_scores = []\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        dbscan_labels = dbscan.fit_predict(features_scaled)\n",
    "        unique_labels = np.unique(dbscan_labels)\n",
    "        if len(unique_labels) > 1:  # Ensure more than one cluster is formed\n",
    "            dbscan_scores.append(davies_bouldin_score(features_scaled, dbscan_labels))\n",
    "\n",
    "if dbscan_scores:\n",
    "    optimal_dbscan_params = [(eps, min_samples) for eps in eps_values for min_samples in min_samples_values][np.argmin(dbscan_scores)]\n",
    "\n",
    "    # Use the optimal parameters to perform clustering\n",
    "    optimal_eps, optimal_min_samples = optimal_dbscan_params\n",
    "    dbscan = DBSCAN(eps=optimal_eps, min_samples=optimal_min_samples)\n",
    "    dbscan_labels = dbscan.fit_predict(features_scaled)\n",
    "\n",
    "    # Evaluate K-Means and DBSCAN\n",
    "    kmeans_silhouette = silhouette_score(features_scaled, kmeans_labels)\n",
    "    dbscan_davies_bouldin = davies_bouldin_score(features_scaled, dbscan_labels)\n",
    "\n",
    "    print(\"Optimal number of clusters for K-Means:\", optimal_kmeans_clusters)\n",
    "    print(\"Optimal parameters for DBSCAN (epsilon, min_samples):\", optimal_dbscan_params)\n",
    "    print(\"K-Means Silhouette Score:\", kmeans_silhouette)\n",
    "    print(\"DBSCAN Davies-Bouldin Score:\", dbscan_davies_bouldin)\n",
    "else:\n",
    "    print(\"DBSCAN failed to generate multiple clusters with the given parameter ranges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcf6e0d-d2de-4661-bf86-882c86f5dfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b631012-c728-4492-9a19-f2cfae4deb41",
   "metadata": {},
   "source": [
    "Task 2 – Use a dimensionality reduction technique before using K-Means and DBSCAN on the dataset.\n",
    "\n",
    "• What is the dimensionality reduction technique that you choose, and why?\n",
    "\n",
    "• Does it have any effect on your code efficiency, both in terms of computational efficiency and clustering output?\n",
    "\n",
    "• How do you compare the outcome of this model with the model where thedimensionality reduction technique was not applied to the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30209dac-92a6-4bbe-94ad-f7955f6f8bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m silhouette_score\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming features_train, features_test, y_train, y_test are loaded from the dataset\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Concatenate train and test features\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([features_train, features_test], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Standardize the features\u001b[39;00m\n\u001b[0;32m     15\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features_train' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Load the dataset\n",
    "# Assuming features_train, features_test, y_train, y_test are loaded from the dataset\n",
    "\n",
    "# Concatenate train and test features\n",
    "features = pd.concat([features_train, features_test], axis=0, ignore_index=True)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Perform clustering using K-Means with PCA-transformed features\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "kmeans.fit(features_pca)\n",
    "kmeans_labels = kmeans.labels_\n",
    "\n",
    "# Perform clustering using DBSCAN with PCA-transformed features\n",
    "dbscan = DBSCAN(eps=3, min_samples=2)\n",
    "dbscan_labels = dbscan.fit_predict(features_pca)\n",
    "\n",
    "# Evaluate K-Means with PCA using Silhouette Score\n",
    "kmeans_silhouette = silhouette_score(features_pca, kmeans_labels)\n",
    "print(\"K-Means with PCA Silhouette Score:\", kmeans_silhouette)\n",
    "\n",
    "# Compare clustering outcomes, computational efficiency, and interpretability with the model where dimensionality reduction was not applied\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8074edbb-5c39-4e34-9360-710538caa514",
   "metadata": {},
   "source": [
    "Task 3 – Visualize your clustering.\n",
    "\n",
    "• Have you applied any dimensionality reduction techniques? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b84a57e-2ece-49ab-ad7c-51cf32300210",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Plot clusters obtained from K-Means with PCA\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m plot_clusters(features_pca, kmeans_labels, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK-Means Clustering with PCA\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Plot clusters obtained from DBSCAN with PCA\u001b[39;00m\n\u001b[0;32m     26\u001b[0m plot_clusters(features_pca, dbscan_labels, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDBSCAN Clustering with PCA\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features_pca' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Visualize clustering results with PCA-transformed features\n",
    "def plot_clusters(features_pca, labels, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if features_pca.shape[1] == 2:\n",
    "        plt.scatter(features_pca[:, 0], features_pca[:, 1], c=labels, cmap='viridis')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "    elif features_pca.shape[1] == 3:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(features_pca[:, 0], features_pca[:, 1], features_pca[:, 2], c=labels, cmap='viridis')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Principal Component 1')\n",
    "        ax.set_ylabel('Principal Component 2')\n",
    "        ax.set_zlabel('Principal Component 3')\n",
    "    plt.show()\n",
    "\n",
    "# Plot clusters obtained from K-Means with PCA\n",
    "plot_clusters(features_pca, kmeans_labels, 'K-Means Clustering with PCA')\n",
    "\n",
    "# Plot clusters obtained from DBSCAN with PCA\n",
    "plot_clusters(features_pca, dbscan_labels, 'DBSCAN Clustering with PCA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4aef9-745d-4aac-ad64-31565b2b25f2",
   "metadata": {},
   "source": [
    "Task 4 – Write a scientific report which includes\n",
    "\n",
    "• Introduction (what is the problem you are solving?)\n",
    "\n",
    "• Data processing (what are the choices you made in data processing and how you performed it?)\n",
    "\n",
    "• Modeling (make sure you have answered all the questions in Tasks 1-3)\n",
    "\n",
    "• Conclusion (How do you interpret the identified clusters? What do they represent? What were the “scientific” bottlenecks? How did you overcome them?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d92167-062f-4f17-9f21-aae2b97cf26a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmpl_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmplot3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Axes3D\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Assuming features_train, features_test, y_train, y_test are loaded from the dataset\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Concatenate train and test features\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([features_train, features_test], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Data processing\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Standardize the features\u001b[39;00m\n\u001b[0;32m     19\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Load the dataset\n",
    "# Assuming features_train, features_test, y_train, y_test are loaded from the dataset\n",
    "\n",
    "# Concatenate train and test features\n",
    "features = pd.concat([features_train, features_test], axis=0, ignore_index=True)\n",
    "\n",
    "# Data processing\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Modeling\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "features_pca = pca.fit_transform(features_scaled)\n",
    "\n",
    "# Perform clustering using K-Means with PCA-transformed features\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "kmeans.fit(features_pca)\n",
    "kmeans_labels = kmeans.labels_\n",
    "\n",
    "# Perform clustering using DBSCAN with PCA-transformed features\n",
    "dbscan = DBSCAN(eps=3, min_samples=2)\n",
    "dbscan_labels = dbscan.fit_predict(features_pca)\n",
    "\n",
    "# Evaluate K-Means with PCA using Silhouette Score\n",
    "kmeans_silhouette = silhouette_score(features_pca, kmeans_labels)\n",
    "\n",
    "# Visualize clustering results with PCA-transformed features\n",
    "def plot_clusters(features_pca, labels, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if features_pca.shape[1] == 2:\n",
    "        plt.scatter(features_pca[:, 0], features_pca[:, 1], c=labels, cmap='viridis')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Principal Component 1')\n",
    "        plt.ylabel('Principal Component 2')\n",
    "    elif features_pca.shape[1] == 3:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(features_pca[:, 0], features_pca[:, 1], features_pca[:, 2], c=labels, cmap='viridis')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Principal Component 1')\n",
    "        ax.set_ylabel('Principal Component 2')\n",
    "        ax.set_zlabel('Principal Component 3')\n",
    "    plt.show()\n",
    "\n",
    "# Plot clusters obtained from K-Means with PCA\n",
    "plot_clusters(features_pca, kmeans_labels, 'K-Means Clustering with PCA')\n",
    "\n",
    "# Plot clusters obtained from DBSCAN with PCA\n",
    "plot_clusters(features_pca, dbscan_labels, 'DBSCAN Clustering with PCA')\n",
    "\n",
    "# Write a scientific report\n",
    "report = \"\"\"\n",
    "Scientific Report: Human Activity Recognition Clustering\n",
    "\n",
    "Introduction:\n",
    "The problem addressed in this study is human activity recognition using smartphone sensor data. The goal is to identify different activities such as walking, walking upstairs, walking downstairs, sitting, standing, and laying based on accelerometer and gyroscope readings from a Samsung Galaxy S II smartphone.\n",
    "\n",
    "Data Processing:\n",
    "The dataset consists of sensor readings captured at a constant rate of 50Hz, preprocessed with noise filters, and segmented into fixed-width sliding windows. Features were extracted from these windows in the time and frequency domains. The data was then standardized using StandardScaler to ensure that all features are on a similar scale.\n",
    "\n",
    "Modeling:\n",
    "We applied dimensionality reduction using Principal Component Analysis (PCA) to reduce the dimensionality of the dataset while retaining 95% of the variance. Clustering was performed using K-Means and DBSCAN algorithms on the PCA-transformed features. For K-Means, we chose 6 clusters based on domain knowledge, while for DBSCAN, we used an epsilon value of 3 and a minimum samples parameter of 2. We evaluated the clustering quality of K-Means using the Silhouette Score.\n",
    "\n",
    "Conclusion:\n",
    "The clustering results reveal distinct clusters representing different activities. Each cluster represents a specific human activity, such as walking, sitting, standing, etc. The identified clusters provide insights into the patterns and structures present in the data, facilitating activity recognition. One scientific bottleneck encountered was the high dimensionality of the dataset, which was addressed by applying PCA for dimensionality reduction. This allowed us to overcome computational complexity and visualize the clustering results effectively in a lower-dimensional space.\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42508c90-250e-4612-b580-97bf2d4be4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
